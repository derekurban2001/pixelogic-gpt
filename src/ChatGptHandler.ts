import { Configuration, OpenAIApi, type CreateChatCompletionRequest } from 'openai-edge'
import { OpenAIStream, StreamingTextResponse } from 'ai'

/**
 * @summary Specifies the GPT model to use for chat completion.
 * @description ID of the model to use. Check the model endpoint compatibility table for details on which models are supported.
 */
export type ChatGptModel = 'gpt-3.5-turbo' | 'gpt-3.5-turbo-16k' | 'gpt-4' | 'gpt-4-32k'

/**
 * @summary Indicates the role of the message author in the chat.
 * @description The role of the messages author can be one of 'system', 'assistant', 'user', or 'function'.
 */
export type ChatGptRole = 'system' | 'assistant' | 'user' | 'function'

/**
 * @summary Provides detailed structure for a function call within a message.
 * @description Object containing the name and arguments for calling a function.
 * @property {string} name - Required: The name of the function to call. Must comply with function name rules.
 * @property {string} arguments - Required: Arguments to call the function with, as generated by the model in JSON format.
 */
export interface ChatGptFunctionCall {
	name: string
	arguments: string
}

/**
 * @summary Describes available functions that the model can generate JSON inputs for.
 * @description Functions that the model may generate inputs for, including function schema and description.
 * @property {string} name - Required: The name of the function, must be alphanumeric or contain underscores and dashes, max length 64.
 * @property {string} [description] - Optional: A description to help the model understand when and how to call the function.
 * @property {Object} parameters - Required: JSON Schema object describing the function's accepted parameters.
 */
export interface ChatGPTFunction {
	name: string
	description?: string
	parameters: object
}

/**
 * @summary Describes the properties of a single message in the chat.
 * @description Each message has an author role, content, and optional fields for function names and calls.
 * @property {ChatGptRole} role - Required: Specifies the role of the author of the message.
 * @property {string | null} content - Required: The contents of the message. May be null if the message involves a function call from the assistant.
 * @property {string} [name] - Optional: The name of the author. Required if role is 'function'.
 * @property {ChatGptFunctionCall} [function_call] - Optional: Details about a function to be called, includes function name and arguments.
 */
export interface ChatGptMessage {
	role: ChatGptRole
	content: string | null
	name?: string
	function_call?: ChatGptFunctionCall
}

/**
 * @summary Describes the complete set of arguments for generating chat completions.
 * @description The main configuration object for chat-based model interactions.
 * @property {ChatGptModel} model - Required: The ID of the model to use.
 * @property {Array<ChatGptMessage>} messages - Required: An array of messages comprising the conversation so far.
 * @property {Array<ChatGPTFunction>} [functions] - Optional: A list of function schemas that the model can interact with.
 * @property {('none' | 'auto' | { name: string })} [function_call] - Optional: Controls how the model responds to function calls. Default is "none" if no functions are present, "auto" otherwise.
 * @property {number | null} [temperature] - Optional: Sampling temperature between 0 and 2. Default is 1.
 * @property {number | null} [top_p] - Optional: Nucleus sampling parameter between 0 and 1. Default is 1.
 * @property {number | null} [n] - Optional: Number of chat completion choices to generate for each input. Default is 1.
 * @property {boolean | null} [stream] - Do Not Set, will be set by 'streamChatCompletion' or 'chatCompletion' accordingly.
 * @property {string | string[] | null} [stop] - Optional: Sequences where the API will stop generating further tokens. Max of 4 sequences.
 * @property {number} [max_tokens] - Optional: Maximum number of tokens to generate. Limited by model's context length.
 * @property {number | null} [presence_penalty] - Optional: Penalty for new topics, between -2.0 and 2.0. Default is 0.
 * @property {number | null} [frequency_penalty] - Optional: Penalty for repetition, between -2.0 and 2.0. Default is 0.
 * @property {Object} [logit_bias] - Optional: JSON object to adjust token likelihood. Token IDs mapped to bias values between -100 and 100.
 * @property {string} [user] - Optional: A unique identifier for the end-user for monitoring and abuse detection.
 */
export interface ChatCompletionArgs {
	model: ChatGptModel
	messages: Array<ChatGptMessage>
	functions?: Array<ChatGPTFunction>
	function_call?: 'none' | 'auto' | { name: string }
	temperature?: number | null
	top_p?: number | null
	n?: number | null
	stream?: boolean | null
	stop?: string | string[] | null
	max_tokens?: number
	presence_penalty?: number | null
	frequency_penalty?: number | null
	logit_bias?: object
	user?: string
}

/**
 * @summary Enumerates the reason the model stopped generating tokens.
 * @description The model can stop generating tokens due to natural stop points, reaching maximum tokens, or making a function call.
 */
export type ChatGptResponseFinishReason = 'stop' | 'length' | 'function_call'

/**
 * @summary Provides statistics for the completion request.
 * @description Shows usage statistics including prompt tokens, completion tokens, and total tokens.
 * @property {number} prompt_tokens - Number of tokens in the prompt.
 * @property {number} completion_tokens - Number of tokens in the generated completion.
 * @property {number} total_tokens - Total number of tokens used in the request (prompt + completion).
 */
export interface ChatGptResponseUsage {
	prompt_tokens: number
	completion_tokens: number
	total_tokens: number
}

/**
 * @summary Represents a single chat completion choice.
 * @description Details of a single choice in the list of chat completion choices.
 * @property {number} index - The index of the choice in the list of choices.
 * @property {ChatGptMessage} message - A chat completion message generated by the model.
 * @property {ChatGptResponseFinishReason} finish_reason - The reason the model stopped generating tokens.
 */
export interface ChatGptResponseChoice {
	index: number
	message: ChatGptMessage
	finish_reason: ChatGptResponseFinishReason
}

/**
 * @summary Describes the complete response for a chat completion.
 * @description The main object representing the complete chat completion result.
 * @property {string} id - A unique identifier for the chat completion.
 * @property {string} object - The object type, always "chat.completion".
 * @property {number} created - A Unix timestamp of when the chat completion was created.
 * @property {string} model - The model used for the chat completion.
 * @property {Array<ChatGptResponseChoice>} choices - A list of chat completion choices.
 * @property {ChatGptResponseUsage} usage - Usage statistics for the completion request.
 */
export interface ChatGptResponse {
	id: string
	object: 'chat.completion'
	created: number
	model: string
	choices: Array<ChatGptResponseChoice>
	usage: ChatGptResponseUsage
}

/**
 * @class
 * @summary Manages interactions with the GPT model via OpenAI API.
 * @description Initializes the API key and provides a method for chat completions.
 */
export default class ChatGptHandler {
	/**
	 * @private
	 * @type {OpenAIApi}
	 */
	private openai: OpenAIApi

	/**
	 * @constructor
	 * @param {string} apiKey - The API key for accessing OpenAI API.
	 * @throws {Error} - Throws an error if an invalid API key is provided or initialization fails.
	 */
	constructor(apiKey: string) {
		try {
			// Validate API key
			if (typeof apiKey !== 'string' || apiKey.trim() === '') {
				throw new Error('Invalid API key provided.')
			}

			// Initialize OpenAI API
			this.openai = new OpenAIApi(
				new Configuration({
					apiKey,
				}),
			)

			// Optionally: Validate that the OpenAI API was initialized successfully
			if (!this.openai || typeof this.openai !== 'object') {
				throw new Error('Failed to initialize OpenAI API.')
			}
		} catch (error) {
			// Log the error and re-throw it
			console.error('Error in constructor:', error)
			throw error
		}
	}

	/**
	 * @method
	 * @async
	 * @summary Handles chat completion requests.
	 * @param {ChatCompletionArgs} args - The chat completion arguments.
	 * @param {{
	 *    onStart: () => void,
	 *    onToken: (token: string) => void,
	 *    onCompletion: () => void
	 * }} [callbacks] - Optional callbacks for various stages of the chat completion.
	 * @returns {Promise<StreamingTextResponse>} Returns a streaming text response.
	 * @throws {Error} - Throws an error if the API request fails, invalid arguments are provided, or streaming fails.
	 */
	public async streamChatCompletion(
		args: ChatCompletionArgs,
		callbacks?: {
			onStart?: () => void
			onToken?: (token: string) => void
			onCompletion?: () => void
		},
	): Promise<StreamingTextResponse> {
		try {
			// Validate input args
			if (!args || typeof args !== 'object') {
				throw new Error('Invalid arguments provided.')
			}

			// Force stream to be true for this method
			args.stream = true

			const response = await this.openai.createChatCompletion(args as CreateChatCompletionRequest)

			// Check if the request was successful
			if (!response.ok) {
				throw new Error(`API request failed with status ${response.status}: ${response.statusText}`)
			}

			const stream = OpenAIStream(response, {
				onStart() {
					if (callbacks?.onStart) callbacks.onStart()
				},
				onToken(token) {
					if (callbacks?.onToken) callbacks.onToken(token)
				},
				onCompletion() {
					if (callbacks?.onCompletion) callbacks.onCompletion()
				},
			})

			// Validate that the stream was created successfully
			if (!stream || typeof stream !== 'object') {
				throw new Error('Failed to create stream.')
			}

			return new StreamingTextResponse(stream, {
				headers: {
					'Content-Type': 'text/event-stream',
					'X-Content-Type-Options': 'nosniff',
				},
			})
		} catch (error) {
			// Log the error and re-throw it
			console.error('Error in streamChatCompletion:', error)
			throw error
		}
	}

	/**
	 * @method
	 * @async
	 * @summary Handles chat completion requests.
	 * @param {ChatCompletionArgs} args - The chat completion arguments.
	 * @returns {Promise<ChatGptResponse>} A promise that resolves with the chat completion response.
	 * @throws {Error} - Throws an error if the API request fails.
	 */
	public async chatCompletion(args: ChatCompletionArgs): Promise<ChatGptResponse> {
		try {
			// Validate input args
			if (!args || typeof args !== 'object') {
				throw new Error('Invalid arguments provided.')
			}

			// Force stream to be false, as this method does not support streaming
			args.stream = false

			const response = await this.openai.createChatCompletion(args as CreateChatCompletionRequest)

			// Check if the request was successful
			if (!response.ok) {
				console.error(await response.json())
				throw new Error(`API request failed with status ${response.status}: ${response.statusText}`)
			}

			// Parse and return the response
			const parsedResponse = await response.json()

			if (!parsedResponse || typeof parsedResponse !== 'object') {
				throw new Error('Received invalid response from the API.')
			}

			return parsedResponse as ChatGptResponse
		} catch (error) {
			// Log the error and re-throw it
			console.error('Error in chatCompletion:', error)
			throw error
		}
	}
}
